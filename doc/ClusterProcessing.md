
These are the steps for (a bit automated) Spiral reconstruction and Blobbing in a Cluster. 

# Requirements
Our cluster is quite primitive so we need some singularity/apptainer containers
* MATLAB container (2021a)
* bart Container
* FSL container
* AFNI container
* SPM12 package
* SpiralReco
* RecoVBVD (Cartesian reconstruction tool)

# Preparation
first, copy all data from iRODS `cp -r <iRODS_mount>/mrdata/<STUDY>/experiments/<EXPERIMENT_ID> /ptmp/<USER>/<path>`. Your measurement folder will have TWIX, EXP_DATA and DICOM folders. Now, copy the contents of '<SpiralReco>/scripts/private/' into our measurement folder for a nice template to work on.

# Get Coil maps and field maps for Reconstruction
Inside the dep folder run first  section [**getDependencies.m**](../scripts/private/dep/getDependices.m). Adapt the script accordingly for your data. The script automatically picks, calculates and reslices the field map for every single measurement we want to reconstruct.  It needs `recoVBVD` in path for reconstruction accelerated mutli-echo GRE raw data. It will write `fm_csm_MeasUIDxxxx.mat` file in your `dep` folder which will be used later for reconstruction. Make sure the B0map acquistion has same center,Normal and inplane rotation for proper interpolation results (We can fix it by switching to nifiti tools). It also write `cfl` files of the calibration data and generate `runcsm2.sh` script. 

As `bart` is not inside the matlab container, we use the generated  `cfl` files and call `bart` container outside using script `runcsm2.sh`. If we are using the reference scan data, there is no need for additional registration. Once the `bart ecalib` is done, we put the coilmaps back to the  `fm_csm_MeasUIDxxxx.mat` file in the dep folder using the second section of [**getDependencies.m**](../scripts/private/dep/getDependices.m).


# Start reconstruction
Go the root folder of the measurement(`/ptmp/<user>/<EXP_PSEUDONYM>`) and start an array jobs (see [**runArrayjob.sh**](../scripts/private/runArrayjob.sh) and [**Arrayscript.m**](../scripts/private/Arrayscript.m) for more details ). If the raw file and  `fm_csm_MeasUIDxxxx.mat` file (only required for accelerated and offresonace correction) exists, the array job will write `mat` files of the reconstrcuted volumes which can be merged later using `MergeScript.m`. we can run as many reconstructions as we want before calling the `Mergescript.m` (see [**runMergeJob.sh**](../scripts/private/runMergeJob.sh) and [**MergeScript.m**](../scripts/private/Mergescript.m) for more info). At this stage, we will have nifti files ready for bobology.

```bash
sbatch runArrayjob.sh raw/*M<MEASUID>* <B0MODE{none,MFI,MTI}>
sbatch runMergeJob.sh
```


# Motion correction 
Refer [DoMoco.sh](../scripts/private/moco/DoMoco.sh). It needs AFNI container.

Motion correction with respect to the base volume
```bash
# collect all runs files in a folder
find proc/ -iname "*MTI*.nii" -exec cp {} moco\ \;

#get first volume (need FSL)
fslroi input.nii vol1.nii 0 1

# Do motion correction of all files in this folder wrt vol.nii
cd moco
./DoMoco.sh vol1.nii

```

Motion correction with respect to the first volume
```bash
# collect all runs files in a folder
find proc/ -iname "*MTI*.nii" -exec cp {} moco\ \;

# Do motion correction of all files in this folder wrt vol.nii
cd moco
./DoMoco.sh vol1.nii

```

# Blobology with FSL

Refer [DoGLM.py](../scripts/private/glm/DoGLM.py). The python script modifies a template design files and change few parameters like input file, TR, resolution, smoothing, paradigm, etc according to the input time series. Load the [tempalte.fsf](../scripts/private/glm/tempalte.fsf) in FSL feat GUI to check or change the template.


Before running FSL FEAT, we need the paradigm in FSL 3 column format from the log generated by pyschopy using [StimFile_script.m](../scripts/private/glm/StimFile_script.m). If both input nifti time series and paradigm file has Measurement ID like `*M<MeasUID>*`. Then, we can collect all files and completely automate blob creation.

```bash
./DoGLLM.py /ptmp/<user>/<EXP_ID>/moco/allmoco /ptmp/<user>/<EXP_ID>/EXPDATA/paradigm/
```
The smoothing kernel size is `smoothfac*resolution`

### Some visualization hacks
Open all blobs in firefox or fsleyes.

```bash
firefox glm/smooth_1p0_*/report_poststats.html
fsleyes  glm/smooth_1p0*/rendered_thresh_zstat1.nii.gz 

# feat scene exists
fsleyes -s feat glm/smooth*.feat
```

# Vessel distance
The vessel distance map is calculated from high resolution GRE volume typically acquired at large TE. Firstly, a vessel binary mask is created by proper filtering like Frangi filtering and thresholding. Then, Eucledian distance to nearest binary vessel mask is calculated for every voxel.

### getting some data
```
dcm2niix -o vessel DICOM/0023_pe_gre_CP_0p4iso_TE16*
```

Execute the matlab script [script_vessel_new.m](../scripts/private/vessel/script_vessel_new.m).

### Steps to be proccessed
* A brain mask is created.
* A sliding mIP(minimum intensity projection) is performed along slice direction to maximize the contrast
* At the moment, 2D frangi filter is applied indvidually on the slices.
* The high pass filtered image is thresholded to create binary mask
* vessel distance is caculated as the eucledian distance to the nearest vessel in binary mask.
* The vessel distance map is exported nifit for further processing.

# Registration
A high resolution MPRAGE (0.8 mm) is registered and rescliced to the SSFP volume space for easy plotting. SPM12 is used for registration. See [coregh_script.m](../scripts/private/reg/coregh_script.m) for more info.

# What else?
* Skull striping
* free surfer segmentation



